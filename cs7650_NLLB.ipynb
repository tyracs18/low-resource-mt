{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPFnmvfPnJit",
        "outputId": "661e2a71-32a6-4138-a53b-b1dd9a4db6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "print(\"Model loaded on\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvuOyL2ZnTIW",
        "outputId": "c78b9fdb-c244-4d5d-c44e-2e6f91690e93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf bleurt\n",
        "!pip install git+https://github.com/google-research/bleurt.git\n",
        "!pip install tensorflow  # BLEURT requires TensorFlow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNLWpUEfqB1o",
        "outputId": "878867f1-9738-4ebe-e6ca-4d63abda56d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/google-research/bleurt.git\n",
            "  Cloning https://github.com/google-research/bleurt.git to /tmp/pip-req-build-dui3odwd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google-research/bleurt.git /tmp/pip-req-build-dui3odwd\n",
            "  Resolved https://github.com/google-research/bleurt.git to commit cebe7e6f996b40910cfaa520a63db47807e3bf5c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.14.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (2.18.0)\n",
            "Requirement already satisfied: tf-slim>=1.1 in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (1.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->BLEURT==0.0.2) (2025.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow->BLEURT==0.0.2) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow->BLEURT==0.0.2) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow->BLEURT==0.0.2) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow->BLEURT==0.0.2) (0.1.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip\n",
        "!unzip bleurt-base-128.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNAkAHMztq2z",
        "outputId": "fa21549c-23c0-47c6-8c30-c5a432b260bf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-23 22:37:31--  https://storage.googleapis.com/bleurt-oss/bleurt-base-128.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.170.207, 142.251.175.207, 74.125.24.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.170.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 405489453 (387M) [application/zip]\n",
            "Saving to: ‘bleurt-base-128.zip.1’\n",
            "\n",
            "bleurt-base-128.zip 100%[===================>] 386.70M  22.9MB/s    in 18s     \n",
            "\n",
            "2025-04-23 22:37:49 (21.2 MB/s) - ‘bleurt-base-128.zip.1’ saved [405489453/405489453]\n",
            "\n",
            "Archive:  bleurt-base-128.zip\n",
            "replace bleurt-base-128/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sacrebleu"
      ],
      "metadata": {
        "id": "eNcmfBswkqhv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from bleurt import score as bleurt_score\n",
        "import sacrebleu\n",
        "\n",
        "# Load model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "# ----- CONFIGURATION -----\n",
        "input_file = \"ne_sin_devtest.jsonl\"           # or sin_ne_devtest.jsonl\n",
        "output_file = \"direct_predicted_si_devtest.txt\"\n",
        "src_lang = \"npi_Deva\"                          # or \"sin_Sinh\" if reversed\n",
        "tgt_lang = \"sin_Sinh\"                          # or \"npi_Deva\" if reversed\n",
        "checkpoint = \"bleurt-base-128\"\n",
        "# --------------------------\n",
        "\n",
        "# Load data\n",
        "src_sents, tgt_refs = [], []\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        src_sents.append(data[\"src\"])\n",
        "        tgt_refs.append(data[\"tgt\"])\n",
        "print(f\"Loaded {len(src_sents)} sentence pairs from {input_file}\")\n",
        "\n",
        "# Translate in batches\n",
        "def translate_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang):\n",
        "    tokenizer.src_lang = src_lang\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "    outputs = model.generate(**inputs, forced_bos_token_id=bos_token_id, max_length=512)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "predictions = []\n",
        "batch_size = 8\n",
        "for i in tqdm(range(0, len(src_sents), batch_size), desc=\"Translating\"):\n",
        "    batch = src_sents[i:i+batch_size]\n",
        "    predictions.extend(translate_batch(batch))\n",
        "\n",
        "# Save output\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in predictions:\n",
        "        f.write(line.strip() + \"\\n\")\n",
        "print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# ----- Evaluation -----\n",
        "# BLEURT\n",
        "scorer = bleurt_score.BleurtScorer(checkpoint)\n",
        "bleurt_scores = scorer.score(references=tgt_refs, candidates=predictions)\n",
        "avg_bleurt = sum(bleurt_scores) / len(bleurt_scores)\n",
        "print(f\"BLEURT score: {avg_bleurt:.4f}\")\n",
        "\n",
        "# BLEU & ChrF\n",
        "bleu = sacrebleu.corpus_bleu(predictions, [tgt_refs]).score\n",
        "chrf = sacrebleu.corpus_chrf(predictions, [tgt_refs]).score\n",
        "print(f\"SacreBLEU:     {bleu:.2f}\")\n",
        "print(f\"ChrF:          {chrf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9mcviD1rGzn",
        "outputId": "8341ac91-eab5-44a0-c36a-9ade784e6d4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n",
            "Loaded 1012 sentence pairs from ne_sin_devtest.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 127/127 [04:46<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions to direct_predicted_si_devtest.txt\n",
            "BLEURT score: 0.4369\n",
            "SacreBLEU:     9.47\n",
            "ChrF:          41.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from bleurt import score as bleurt_score\n",
        "import sacrebleu\n",
        "\n",
        "# Load model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "# ----- CONFIGURATION -----\n",
        "input_file = \"ne_sin_dev.jsonl\"           # or sin_ne_devtest.jsonl\n",
        "output_file = \"direct_predicted_si_dev.txt\"\n",
        "src_lang = \"npi_Deva\"                          # or \"sin_Sinh\" if reversed\n",
        "tgt_lang = \"sin_Sinh\"                          # or \"npi_Deva\" if reversed\n",
        "checkpoint = \"bleurt-base-128\"\n",
        "# --------------------------\n",
        "\n",
        "# Load data\n",
        "src_sents, tgt_refs = [], []\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        src_sents.append(data[\"src\"])\n",
        "        tgt_refs.append(data[\"tgt\"])\n",
        "print(f\"Loaded {len(src_sents)} sentence pairs from {input_file}\")\n",
        "\n",
        "# Translate in batches\n",
        "def translate_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang):\n",
        "    tokenizer.src_lang = src_lang\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "    outputs = model.generate(**inputs, forced_bos_token_id=bos_token_id, max_length=512)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "predictions = []\n",
        "batch_size = 8\n",
        "for i in tqdm(range(0, len(src_sents), batch_size), desc=\"Translating\"):\n",
        "    batch = src_sents[i:i+batch_size]\n",
        "    predictions.extend(translate_batch(batch))\n",
        "\n",
        "# Save output\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in predictions:\n",
        "        f.write(line.strip() + \"\\n\")\n",
        "print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# ----- Evaluation -----\n",
        "# BLEURT\n",
        "scorer = bleurt_score.BleurtScorer(checkpoint)\n",
        "bleurt_scores = scorer.score(references=tgt_refs, candidates=predictions)\n",
        "avg_bleurt = sum(bleurt_scores) / len(bleurt_scores)\n",
        "print(f\"BLEURT score: {avg_bleurt:.4f}\")\n",
        "\n",
        "# BLEU & ChrF\n",
        "bleu = sacrebleu.corpus_bleu(predictions, [tgt_refs]).score\n",
        "chrf = sacrebleu.corpus_chrf(predictions, [tgt_refs]).score\n",
        "print(f\"SacreBLEU:     {bleu:.2f}\")\n",
        "print(f\"ChrF:          {chrf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f53962-5db3-4072-b6ae-05d989f05411",
        "id": "O1uX1ouJspuk"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n",
            "Loaded 997 sentence pairs from ne_sin_dev.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 125/125 [05:06<00:00,  2.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions to direct_predicted_si_dev.txt\n",
            "BLEURT score: 0.4267\n",
            "SacreBLEU:     8.92\n",
            "ChrF:          40.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from bleurt import score as bleurt_score\n",
        "import sacrebleu\n",
        "\n",
        "# Load model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "# ----- CONFIGURATION -----\n",
        "input_file = \"sin_ne_devtest.jsonl\"           # or sin_ne_devtest.jsonl\n",
        "output_file = \"direct_predicted_ne_devtest.txt\"\n",
        "src_lang = \"sin_Sinh\"   # Input is Sinhala\n",
        "tgt_lang = \"npi_Deva\"   # Output should be Nepali\n",
        "checkpoint = \"bleurt-base-128\"\n",
        "# --------------------------\n",
        "\n",
        "# Load data\n",
        "src_sents, tgt_refs = [], []\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        src_sents.append(data[\"src\"])\n",
        "        tgt_refs.append(data[\"tgt\"])\n",
        "print(f\"Loaded {len(src_sents)} sentence pairs from {input_file}\")\n",
        "\n",
        "# Translate in batches\n",
        "def translate_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang):\n",
        "    tokenizer.src_lang = src_lang\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "    outputs = model.generate(**inputs, forced_bos_token_id=bos_token_id, max_length=512)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "predictions = []\n",
        "batch_size = 8\n",
        "for i in tqdm(range(0, len(src_sents), batch_size), desc=\"Translating\"):\n",
        "    batch = src_sents[i:i+batch_size]\n",
        "    predictions.extend(translate_batch(batch))\n",
        "\n",
        "# Save output\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in predictions:\n",
        "        f.write(line.strip() + \"\\n\")\n",
        "print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# ----- Evaluation -----\n",
        "# BLEURT\n",
        "scorer = bleurt_score.BleurtScorer(checkpoint)\n",
        "bleurt_scores = scorer.score(references=tgt_refs, candidates=predictions)\n",
        "avg_bleurt = sum(bleurt_scores) / len(bleurt_scores)\n",
        "print(f\"BLEURT score: {avg_bleurt:.4f}\")\n",
        "\n",
        "# BLEU & ChrF\n",
        "bleu = sacrebleu.corpus_bleu(predictions, [tgt_refs]).score\n",
        "chrf = sacrebleu.corpus_chrf(predictions, [tgt_refs]).score\n",
        "print(f\"SacreBLEU:     {bleu:.2f}\")\n",
        "print(f\"ChrF:          {chrf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfaa3bbb-ae9a-4b0f-9678-ce303f49e20f",
        "id": "hDlCemNKsvqg"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n",
            "Loaded 1012 sentence pairs from sin_ne_devtest.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 127/127 [03:21<00:00,  1.59s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions to direct_predicted_ne_devtest.txt\n",
            "BLEURT score: 0.0924\n",
            "SacreBLEU:     8.44\n",
            "ChrF:          42.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "from bleurt import score as bleurt_score\n",
        "import sacrebleu\n",
        "\n",
        "# Load model\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"Model loaded on {device}\")\n",
        "\n",
        "# ----- CONFIGURATION -----\n",
        "input_file = \"sin_ne_dev.jsonl\"           # or sin_ne_devtest.jsonl\n",
        "output_file = \"direct_predicted_ne_dev.txt\"\n",
        "src_lang = \"sin_Sinh\"   # Input is Sinhala\n",
        "tgt_lang = \"npi_Deva\"   # Output should be Nepali\n",
        "checkpoint = \"bleurt-base-128\"\n",
        "# --------------------------\n",
        "\n",
        "# Load data\n",
        "src_sents, tgt_refs = [], []\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        src_sents.append(data[\"src\"])\n",
        "        tgt_refs.append(data[\"tgt\"])\n",
        "print(f\"Loaded {len(src_sents)} sentence pairs from {input_file}\")\n",
        "\n",
        "# Translate in batches\n",
        "def translate_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang):\n",
        "    tokenizer.src_lang = src_lang\n",
        "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "    outputs = model.generate(**inputs, forced_bos_token_id=bos_token_id, max_length=512)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "predictions = []\n",
        "batch_size = 8\n",
        "for i in tqdm(range(0, len(src_sents), batch_size), desc=\"Translating\"):\n",
        "    batch = src_sents[i:i+batch_size]\n",
        "    predictions.extend(translate_batch(batch))\n",
        "\n",
        "# Save output\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in predictions:\n",
        "        f.write(line.strip() + \"\\n\")\n",
        "print(f\"Saved predictions to {output_file}\")\n",
        "\n",
        "# ----- Evaluation -----\n",
        "# BLEURT\n",
        "scorer = bleurt_score.BleurtScorer(checkpoint)\n",
        "bleurt_scores = scorer.score(references=tgt_refs, candidates=predictions)\n",
        "avg_bleurt = sum(bleurt_scores) / len(bleurt_scores)\n",
        "print(f\"BLEURT score: {avg_bleurt:.4f}\")\n",
        "\n",
        "# BLEU & ChrF\n",
        "bleu = sacrebleu.corpus_bleu(predictions, [tgt_refs]).score\n",
        "chrf = sacrebleu.corpus_chrf(predictions, [tgt_refs]).score\n",
        "print(f\"SacreBLEU:     {bleu:.2f}\")\n",
        "print(f\"ChrF:          {chrf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae46a730-2148-4913-bdf7-53d06ab747f6",
        "id": "OgK0YCQVs1jl"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda\n",
            "Loaded 997 sentence pairs from sin_ne_dev.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Translating: 100%|██████████| 125/125 [02:44<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions to direct_predicted_ne_dev.txt\n",
            "BLEURT score: 0.1012\n",
            "SacreBLEU:     8.43\n",
            "ChrF:          43.42\n"
          ]
        }
      ]
    }
  ]
}